{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем необходимые библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявляем api ключ, видеоролики, колонки data frame и то, что у нас русские слова. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"russian\")\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "api_key = \"AIzaSyCJk-C6RJfzZahZSiuTg4Ahs-xGjKF2jMo\"\n",
    "video_id = [\"yMBHihjcgaA\", \"-kicGBIh5dw\", \"70JDd-Ywnio\", \"37v1PJqG06g\", \"7hQqaEvXPho\", \"m13BBD6wAFQ\", \"FAl7z9w0e5k\",\n",
    "            \"6LEUTrA5NrE\", \"isW8jWoEPGE\", \"dFeimaTDX84\"]\n",
    "col = ['Author', 'Text', 'Likes', 'Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мной было рассмотрено 10 последних видео с научно-развлекательного канала Utopia Show.\n",
    "Видео формата:\n",
    "<br>1)Рассказ о фактах, на которые многие люди не наткнулись бы.\n",
    "<br>2)Рассказы о мифах, которые многие люди считают правдой.\n",
    "<br>3)Интерпретация научных фактов для широкой аудитории.\n",
    "<br>4)Рассказ о фактах, на которые многие люди не наткнулись бы.\n",
    "<br>5)Интерпретация научных фактов для широкой аудитории.\n",
    "<br>6)Рассказ об истории из прошлого.\n",
    "<br>7)Рассказ о фактах, на которые многие люди не наткнулись бы.\n",
    "<br>8)Интерпретация научных фактов для широкой аудитории.\n",
    "<br>9)Рассказы о мифах, которые многие люди  считают правдой.\n",
    "<br>10)Рассказ об истории из прошлого."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С каждого видео берем первые 100 комментариев при помощи api и возвращаем data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def df_download(dataFrame):\n",
    "    for v_id in video_id:  # Проходимся по видео\n",
    "        request = resource.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=v_id,\n",
    "            maxResults=100,\n",
    "            order=\"orderUnspecified\")\n",
    "        response = request.execute()\n",
    "        items = response[\"items\"][:1000]\n",
    "        for item in items:  # Проходимся по комментариям\n",
    "            item_info = item[\"snippet\"]\n",
    "            line = []\n",
    "            topLevelComment = item_info[\"topLevelComment\"]\n",
    "            comment_info = topLevelComment[\"snippet\"]\n",
    "            line.append(comment_info[\"authorDisplayName\"])\n",
    "            line.append(comment_info[\"textDisplay\"])\n",
    "            line.append(comment_info[\"likeCount\"])\n",
    "            line.append(comment_info[\"publishedAt\"])\n",
    "            y_data.append(line)\n",
    "    dataFrame = pd.DataFrame(data=y_data)  # Записываем результат в датафрейм\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Функция подсчета tf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_calc(dictionary, sum_nk):\n",
    "    local_dict = {}\n",
    "    for word, counts in dictionary.items():\n",
    "        local_dict[word] = counts / sum_nk  # Делим количество вхождений на количество слов.\n",
    "    return local_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Производим очистку комментариев: удаляем пунктуацию, приводим к нижнему регистру, делаем стемминг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_manipulation(dataFrame):\n",
    "    index = 0\n",
    "    for string in df['Text']:  # Проходимся по тексту комментариев\n",
    "        df.loc[index, 'Text'] = re.sub(\"[^А-Яа-я0-9 ё]\", \"\", string).lower()  # Удаляем ненужные символы\n",
    "        my_string = df.loc[index, 'Text'].split()  # Возвращаем формат\n",
    "\n",
    "        final_list = [word for word in my_string if word not in stop_words]  # Проверяем стоп-слова\n",
    "        final = [stemmer.stem(word) for word in final_list]  # Стемминг\n",
    "        df.loc[index, 'Text'] = ' '.join(final)  # И опять возвращаем формат, так как работали не со string, а с char[]\n",
    "        index += 1\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вызываем все функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All comments in BeforeChange.csv\n",
      "Edited comments in AfterChange.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    resource = build('youtube', 'v3', developerKey=api_key)\n",
    "    word_dict = {}\n",
    "    arr = []\n",
    "    y_data = []\n",
    "    _len = 0\n",
    "    this_key = \"\"\n",
    "\n",
    "    df = pd.DataFrame(data=y_data)\n",
    "    df = df_download(df)  #  Функция загрузки\n",
    "    df.columns = col  # Обьявляем колонки\n",
    "    df.to_csv('BeforeChange.csv', index=False, header=True)  #  Запись в файл\n",
    "    print(\"All comments in BeforeChange.csv\")  # Все комментарии будут загружены в файл BeforeChange.csv\n",
    "    df = df_manipulation(df)  #  Функция изменения\n",
    "    df.to_csv('AfterChange.csv', index=False, header=True)  #  Запись в файл\n",
    "    print(\"Edited comments in AfterChange.csv\")  # Записываем получившиеся в AfterChange.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразование мешок слов и tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    for comments in df['Text']:  # Пробегаемся по тексту комментариев\n",
    "        comments = comments.split()\n",
    "        word_set = set(comments)\n",
    "        dict_comm = dict.fromkeys(word_set, 0)\n",
    "        for word in comments:  # Проверяем на наличие слова в словаре\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = 0  # Ставим нули если ещё не в словаре\n",
    "        for word in comments:  # Считаем количество\n",
    "            word_dict[word] += 1\n",
    "            dict_comm[word] += 1\n",
    "        arr.append(dict_comm)\n",
    "\n",
    "    _len = len(word_dict.keys())\n",
    "    tf = tf_calc(word_dict, _len)  # Вызываем функцию по словарю и длине\n",
    "\n",
    "    #  Создание словарей с нулями\n",
    "    idf = dict.fromkeys(word_dict.keys(), 0)\n",
    "    tfidf = dict.fromkeys(word_dict.keys(), 0)\n",
    "\n",
    "    for word in word_dict.keys():  # Снова проходим\n",
    "        word_counts = 0\n",
    "        for dict_comm in arr:  # По массиву словарей\n",
    "            if word in dict_comm:\n",
    "                word_counts += 1\n",
    "        idf[word] = math.log(len(arr) / word_counts)  #  log длины массива на количество вхождений\n",
    "\n",
    "    for word in word_dict:\n",
    "        tfidf[word] = tf[word] * idf[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запись мешка слов и tfidf в файлы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag words in bag.txt\n",
      "tfidf in tfidf.txt\n",
      "Success.\n"
     ]
    }
   ],
   "source": [
    "    with open('bag.txt', 'w') as file:  #  Запись мешка слов\n",
    "        for key, value in tfidf.items():\n",
    "            file.write(f'{key}\\n')\n",
    "    print(\"bag words in bag.txt\")\n",
    "\n",
    "    with open('tfidf.txt', 'w') as file:  #  Запись tfidf\n",
    "        for key, value in tfidf.items():\n",
    "            file.write(f'{key}, {value}\\n')\n",
    "    print(\"tfidf in tfidf.txt\")\n",
    "\n",
    "    \"\"\" Uncomment this if not need numbers as separate words\n",
    "    with open('tfidf.txt', 'w') as file:\n",
    "        for key, value in tfidf.items():\n",
    "            if not key.isdigit():\n",
    "                file.write(f'{key}, {value}\\n')\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Success.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
